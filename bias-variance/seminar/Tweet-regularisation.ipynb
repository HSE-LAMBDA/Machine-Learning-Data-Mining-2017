{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Please, load data from https://inclass.kaggle.com/c/si650winter11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c9924177951c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "with open('./training.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split('\\t')\n",
    "        label, tweet = tokens[0], tokens[1:]\n",
    "        labels.append(int(label))\n",
    "        data.append('\\t'.join(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonyms were obtained by searching for top 20 most similar words accoroding to Glove twitter model: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('synonyms.json') as f:\n",
    "    synonyms = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word a list with at most 20 words is provided. Along with synonyms similarity coefficients are given (the large coefficient the more similar the synonym is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dude', 0.7253455519676208],\n",
       " ['boy', 0.7174534797668457],\n",
       " ['guy', 0.6998012065887451],\n",
       " ['shit', 0.6852256655693054],\n",
       " ['was', 0.6779443621635437],\n",
       " [\"'s\", 0.6762615442276001],\n",
       " ['bad', 0.6734011769294739],\n",
       " ['men', 0.6714873909950256],\n",
       " ['that', 0.6709873080253601],\n",
       " ['lol', 0.6646672487258911],\n",
       " ['fuck', 0.6629763841629028],\n",
       " ['woman', 0.6613606214523315],\n",
       " ['but', 0.6589959263801575],\n",
       " ['bro', 0.6560863256454468],\n",
       " ['way', 0.6556506752967834],\n",
       " ['girl', 0.6554247736930847],\n",
       " ['nigga', 0.6549790501594543],\n",
       " ['who', 0.6531668305397034],\n",
       " ['just', 0.6512974500656128],\n",
       " ['so', 0.6502212882041931]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all words used in the dataset\n",
    "\n",
    "Here you can do some feature selection e.g. by word frequency or by tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(data)\n",
    "vocab = list(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select some informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### change me!\n",
    "pruned_vocab = vocab[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend them with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CHANGE ME\n",
    "extended_vocab = pruned_vocab + [ synonyms[w][0][0] for w in pruned_vocab if w in synonyms ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extended_vocab_set = set(extended_vocab)\n",
    "\n",
    "### to exclude repeating words\n",
    "extended_vocab = list(extended_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index = dict(zip(\n",
    "    extended_vocab,\n",
    "    range(len(extended_vocab))\n",
    "))\n",
    "\n",
    "index_to_word = dict(zip(\n",
    "    range(len(extended_vocab)),\n",
    "    extended_vocab,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(vocabulary=extended_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing links between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_ids1 = []\n",
    "word_ids2 = []\n",
    "\n",
    "for w1 in extended_vocab:\n",
    "    if w1 not in synonyms:\n",
    "        continue\n",
    "\n",
    "    for w2, _ in synonyms[w1]:\n",
    "        if w2 in extended_vocab_set:\n",
    "            word_ids1.append(word_to_index[w1])\n",
    "            word_ids2.append(word_to_index[w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_ids1 = np.array(word_ids1)\n",
    "word_ids2 = np.array(word_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(word_ids1)\n",
    "print(word_ids2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ypu don't have theano or lasagne, please, follow\n",
    "\n",
    "http://lasagne.readthedocs.io/en/latest/user/installation.html#bleeding-edge-version\n",
    "\n",
    "```\n",
    "pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = T.fmatrix()\n",
    "y = T.fvector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = len(extended_vocab)\n",
    "\n",
    "W = theano.shared(\n",
    "    ### don't do such initialization in neural networks!!!\n",
    "    np.random.uniform(-1.0e-2, 1e-2, size=(n_features, 1)).astype('float32')\n",
    ")\n",
    "\n",
    "b = theano.shared(np.float32(0.0))\n",
    "\n",
    "link_words_1 = theano.shared(word_ids1.astype('int64'))\n",
    "link_words_2 = theano.shared(word_ids2.astype('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = T.flatten(\n",
    "    T.nnet.sigmoid(T.dot(X, W) + b),\n",
    "    ndim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss = -T.mean(\n",
    "    y * T.log(predictions) + (1 - y) * T.log(1 - predictions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_regularisation = 1.0e-2 * T.sum(W ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_regularisation.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And ... our exotic regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Change me!\n",
    "synonim_regularization = 1.0e-2 * T.sum(W ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonim_regularization.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Adjust my coefficients!\n",
    "loss = cross_entropy_loss + l2_regularisation + synonim_regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training part.\n",
    "\n",
    "If you don't have lasagne just copy adamax optimizer from:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.updates import adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = theano.function(\n",
    "    inputs=[X, y],\n",
    "    outputs=[cross_entropy_loss, l2_regularisation, synonim_regularization],\n",
    "    updates=adamax(loss, [W, b])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_seq(n_samples, batch_size=128, allow_smaller=False):\n",
    "  indx = np.random.permutation(n_samples)\n",
    "\n",
    "  n_batches = n_samples // batch_size + (1 if (n_samples % batch_size != 0) and allow_smaller else 0)\n",
    "\n",
    "  for i in range(n_batches):\n",
    "    i_from = i * batch_size\n",
    "    i_to = i_from + batch_size\n",
    "    yield indx[i_from:i_to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epoches = 16\n",
    "batch_size = 8\n",
    "n_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "losses = np.ndarray(shape=(n_epoches, n_batches), dtype='float32')\n",
    "l2_reg = np.ndarray(shape=(n_epoches, n_batches), dtype='float32')\n",
    "synonym_reg = np.ndarray(shape=(n_epoches, n_batches), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nn_watcher import SNNWatcher\n",
    "\n",
    "watcher = SNNWatcher(\n",
    "    title='LogReg-tweets',\n",
    "    colors=('blue', 'green', 'red'),\n",
    "    labels=('cross-entropy loss', 'l2', 'synonym'),\n",
    "    mode='fill',\n",
    ")\n",
    "\n",
    "for i in range(n_epoches):\n",
    "    for j, indx in enumerate(random_seq(X_train.shape[0], batch_size=batch_size)):\n",
    "        losses[i, j], l2_reg[i, j], synonym_reg[i, j] = train(\n",
    "            X_train[indx].astype('float32'),\n",
    "            y_train[indx].astype('float32')\n",
    "        )\n",
    "    watcher.draw(\n",
    "        losses[:(i + 1)],\n",
    "        l2_reg[:(i + 1)],\n",
    "        synonym_reg[:(i + 1)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it\n",
    "\n",
    "- test your initial vocab;\n",
    "- test your extended vocab without fancy regularization;\n",
    "- test your extended vocab with fancy regularization;\n",
    "- adjust regularization coefficients to obtain best score (don't forget to make train/val/test split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
